{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import pyodbc\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading Excel Sheet Data - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_merge_stage(user_saydate):\n",
    "    \n",
    "    data_dpm = pd.read_excel(f'C:/SCHOOLWORK/Y3 SEM 1/DSCP/Assignment 3 - Sprint 3/Preprocessing/Cleaned Estate Office Data/DPM/{user_saydate}.xlsx')\n",
    "    data_inv = pd.read_excel(f'C:/SCHOOLWORK/Y3 SEM 1/DSCP/Assignment 3 - Sprint 3/Preprocessing/Cleaned Estate Office Data/INV/{user_saydate}.xlsx')\n",
    "\n",
    "    # Merging the dataframes\n",
    "    merged_data = pd.merge(data_dpm, data_inv, on=['Date and Time', 'Location Code'], suffixes=('_dpm', '_inv'))\n",
    "\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_preprocess(user_saydate):\n",
    "    \n",
    "    merged_data = pre_merge_stage(user_saydate)\n",
    "\n",
    "    columns = ['Date and Time', 'PanelCode','BlockNo','Energy kWh_dpm', 'Expected Value kWh','Energy kWh_inv','IRR Value W/m²', 'PR %','DPMSensorIssue','INVSensorIssue']\n",
    "    merged_data['BlockNo'] = merged_data['Location Code'].str.extract(r'(\\d{2})')\n",
    "    merged_data['SensorNo'] = merged_data['Sensor ID'].str.extract(r'(\\d{2})$')\n",
    "    merged_data['PanelCode'] = merged_data['BlockNo'] + '-' + merged_data['SensorNo']\n",
    "    merged_data['INVSensorIssue'] = merged_data.apply(lambda row: 'Issue' if row['Energy kWh_dpm'] > row['Energy kWh_inv'] else 'No Issue', axis=1)\n",
    "    sorted_data = merged_data.sort_values(by=['Date and Time', 'SensorNo'])\n",
    "\n",
    "    cleaned_merged = sorted_data[columns]\n",
    "    cleaned_merged = cleaned_merged.rename(columns={'Date and Time': 'RecordDate', 'Energy kWh_dpm': 'DPM','Expected Value kWh': 'ExpectedDPM', 'Energy kWh_inv': 'INV','IRR Value W/m²': 'IRR','PR %': 'PR',})\n",
    "\n",
    "    return cleaned_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Weather Data - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_date_retrieval(data_file):\n",
    "    data = pd.read_excel(f'./{data_file}.xlsx')\n",
    "    data_date_amend = data.copy()\n",
    "    data_date_amend['Date and Time'] = pd.to_datetime(data_date_amend['Date and Time'])\n",
    "\n",
    "    earliest_date = data_date_amend['Date and Time'].min().date().strftime('%Y-%m-%d')\n",
    "    latest_date = data_date_amend['Date and Time'].max().date().strftime('%Y-%m-%d')\n",
    "\n",
    "    return earliest_date, latest_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_data_retrieval(user_input):\n",
    "\n",
    "  earliest_date, latest_date = api_date_retrieval(user_input)\n",
    "  \n",
    "  try: \n",
    "    url = f\"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/Clementi,Singapore/{earliest_date}/{latest_date}?unitGroup=us&key=MAEVMCSQZL8TPWXXYV5FQSLBJ&contentType=json\"\n",
    "    ResultBytes = urllib.request.urlopen(url)\n",
    "\n",
    "    jsonData = json.load(ResultBytes)\n",
    "          \n",
    "  except urllib.error.HTTPError  as e:\n",
    "    ErrorInfo= e.read().decode() \n",
    "    print('Error code: ', e.code, ErrorInfo)\n",
    "    sys.exit()\n",
    "  except  urllib.error.URLError as e:\n",
    "    ErrorInfo= e.read().decode() \n",
    "    print('Error code: ', e.code,ErrorInfo)\n",
    "    sys.exit()\n",
    "\n",
    "  return jsonData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_data_preprocessing(ask_user_date):\n",
    "\n",
    "    some_data = api_data_retrieval(ask_user_date)\n",
    "\n",
    "    columns_to_extract = ['datetime', 'conditions', 'temp', 'tempmax', 'tempmin', 'precip','cloudcover','solarenergy',\n",
    "                        'solarradiation', 'windspeed', 'uvindex', 'humidity', 'dew', 'pressure', 'visibility']\n",
    "\n",
    "    weather_data = some_data['days'] if 'days' in some_data else some_data\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df = pd.json_normalize(weather_data)\n",
    "    extracted_df = df[columns_to_extract]\n",
    "    extracted_df['temp'] = (extracted_df['temp'] - 32) * (5/9)\n",
    "    extracted_df = extracted_df.round(2)\n",
    "    extracted_df['uvindex'] = extracted_df['uvindex'].astype(int)\n",
    "    extracted_df = extracted_df.rename(columns={'datetime': 'RecordDate', 'conditions': 'Conditions','temp': 'Temp', 'tempmax': 'TempMax','tempmin': 'TempMin', 'precip': 'Precipitation',\n",
    "                                                'cloudcover': 'CloudCover', 'solarenergy': 'SolarEnergy','solarradiation': 'SolarRadiation', 'windspeed': 'WindSpeed','uvindex': 'UVIndex',\n",
    "                                                'humidity': 'Humidity','dew': 'Dew', 'pressure': 'Pressure','visibility': 'Visibility'})\n",
    "\n",
    "    return extracted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Both Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(user_types_data):\n",
    "    \n",
    "    main_data = merged_preprocess(user_types_data)\n",
    "    weather_data = api_data_preprocessing(user_types_data)\n",
    "    final_data = pd.merge(main_data, weather_data, on=['RecordDate'])\n",
    "\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Storage via Azure Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully written to Azure SQL Database.\n"
     ]
    }
   ],
   "source": [
    "df_to_azure = preprocess_data('Feb 2022')\n",
    "\n",
    "server = 'dscpserver.database.windows.net'\n",
    "database = 'DSCPDatabase'\n",
    "username = 'admindscp'\n",
    "password = 'T01b26121gH'\n",
    "driver = 'ODBC Driver 17 for SQL Server'\n",
    "\n",
    "conn_str = f'DRIVER={{{driver}}};SERVER={server};DATABASE={database};UID={username};PWD={password}'\n",
    "conn = pyodbc.connect(conn_str)\n",
    "engine = create_engine(f'mssql+pyodbc:///?odbc_connect={conn_str}')\n",
    "\n",
    "table_name = 'newTable'\n",
    "df_to_azure.to_sql(table_name, engine, index=False, if_exists='append')\n",
    "\n",
    "print(\"DataFrame successfully written to Azure SQL Database.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
